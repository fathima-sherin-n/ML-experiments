{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0ab50f15-bccc-44f1-baea-5ba228fd199a",
      "metadata": {
        "id": "0ab50f15-bccc-44f1-baea-5ba228fd199a",
        "outputId": "7d2b75dc-dfc1-4090-8290-4e1546ad4b09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0607f3d9-6021-4c11-b2ac-50a11417dd18",
      "metadata": {
        "id": "0607f3d9-6021-4c11-b2ac-50a11417dd18",
        "outputId": "a1c67881-0c2c-4ca2-9723-59dbf9a32f02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tf-keras) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "10b34794-5795-47ab-8450-1b887523e6d4",
      "metadata": {
        "id": "10b34794-5795-47ab-8450-1b887523e6d4",
        "outputId": "956b4993-0ae4-421e-ca06-52c45aca2383",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "import random\n",
        "\n",
        "nltk.download(\"punkt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6795452e-334b-44fb-8447-49c66b0c4e60",
      "metadata": {
        "id": "6795452e-334b-44fb-8447-49c66b0c4e60",
        "outputId": "789b4a00-f3fe-4302-db12-05c71f90cb24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ebc54680-2699-438b-9c39-0856b6cb0a1f",
      "metadata": {
        "id": "ebc54680-2699-438b-9c39-0856b6cb0a1f",
        "outputId": "c3a906f6-0a9b-4552-f2bd-65cd3507ef17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Undersampled Dataset Size: 2871\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\",'3.0.0')\n",
        "\n",
        "sample_fraction = 0.01\n",
        "\n",
        "train_data_list = list(dataset[\"train\"])\n",
        "undersampled_data = random.sample(train_data_list, int(len(train_data_list) * sample_fraction))\n",
        "\n",
        "print(\"\\n Undersampled Dataset Size:\", len(undersampled_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8bdcec1e-eee7-489f-8255-0a8a0a87ce74",
      "metadata": {
        "id": "8bdcec1e-eee7-489f-8255-0a8a0a87ce74",
        "outputId": "fd2390b1-ed30-4ad7-ca75-a3a377106364",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "946125b5-5117-4797-b8e1-ace1d3549a47",
      "metadata": {
        "id": "946125b5-5117-4797-b8e1-ace1d3549a47",
        "outputId": "9ec40a51-181a-4e29-f8c6-85e1a68559ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Sample Article:\n",
            " (CNN) -- Yes, it's great to travel light. Sure, too much tech can make life trickier, not easier. No, that's not going to stop us listing some of the coolest gadgets, gizmos and accessories that could just make you the happiest traveler this side of the Apple store. (If only till you lose them/have them stolen.) Narrative clip-on camera . This is a tiny five-megapixel camera that clips onto your clothes and does the work for you, automatically taking two photos every minute when turned on. The a\n",
            "\n",
            "Sample Summary:\n",
            " Narrative is a hands-free (it clips to your clothes) camera that snaps pics automatically every 30 seconds .\n",
            "SleepPhones are headphones in a headband -- perfect for falling asleep to tunes without your bulky earpiece falling out .\n",
            "LV's shower in a trunk is a bit ambitious, not to say impractical, but what an eye-catcher .\n",
            "Steripen Ultra kills bacteria in water in 48 seconds .\n",
            "\n",
            " Tokenized Sentences:\n",
            " [\"(CNN) -- Yes, it's great to travel light.\", 'Sure, too much tech can make life trickier, not easier.', \"No, that's not going to stop us listing some of the coolest gadgets, gizmos and accessories that could just make you the happiest traveler this side of the Apple store.\", '(If only till you lose them/have them stolen.)', 'Narrative clip-on camera .']\n"
          ]
        }
      ],
      "source": [
        "# Extract a sample article and summary\n",
        "sample_article = undersampled_data[0][\"article\"]\n",
        "sample_summary = undersampled_data[0][\"highlights\"]\n",
        "\n",
        "print(\"\\n Sample Article:\\n\", sample_article[:500])  # Display first 500 characters\n",
        "print(\"\\nSample Summary:\\n\", sample_summary)\n",
        "\n",
        "# Tokenize article into sentences\n",
        "def preprocess_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "sentences = preprocess_text(sample_article)\n",
        "print(\"\\n Tokenized Sentences:\\n\", sentences[:5])  # Display first 5 sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a4d0a63d-b6ac-4ba3-9de9-bfbd10bf5974",
      "metadata": {
        "id": "a4d0a63d-b6ac-4ba3-9de9-bfbd10bf5974",
        "outputId": "9d67acfe-1a32-4253-b573-033087d37125",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix Shape: (104, 647)\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "sentence_vectors = vectorizer.fit_transform(sentences).toarray()\n",
        "\n",
        "print(\"\\nTF-IDF Matrix Shape:\", sentence_vectors.shape)  # Shape of TF-IDF matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5bdf9171-e80b-46d6-b0a2-af4ecf07e766",
      "metadata": {
        "id": "5bdf9171-e80b-46d6-b0a2-af4ecf07e766",
        "outputId": "8d3b3c38-1efa-43f6-dbd3-1d57a50da04c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMSummarizer(\n",
            "  (lstm): LSTM(647, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class LSTMSummarizer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super(LSTMSummarizer, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # Output single score per sentence\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        scores = self.fc(lstm_out[:, -1, :])  # Use last hidden state\n",
        "        return scores\n",
        "\n",
        "# Model parameters\n",
        "input_dim = sentence_vectors.shape[1]\n",
        "hidden_dim = 128\n",
        "num_layers = 1\n",
        "\n",
        "# Initialize model\n",
        "lstm_model = LSTMSummarizer(input_dim, hidden_dim, num_layers)\n",
        "print(lstm_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "13a0a4f2-5516-4f9b-9d7f-810fdb20c366",
      "metadata": {
        "id": "13a0a4f2-5516-4f9b-9d7f-810fdb20c366",
        "outputId": "68af74b2-b24b-40e5-c43d-d1875d72f207",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6780\n",
            "Epoch 2/10, Loss: 0.6307\n",
            "Epoch 3/10, Loss: 0.5821\n",
            "Epoch 4/10, Loss: 0.5286\n",
            "Epoch 5/10, Loss: 0.4694\n",
            "Epoch 6/10, Loss: 0.4057\n",
            "Epoch 7/10, Loss: 0.3405\n",
            "Epoch 8/10, Loss: 0.2776\n",
            "Epoch 9/10, Loss: 0.2204\n",
            "Epoch 10/10, Loss: 0.1715\n"
          ]
        }
      ],
      "source": [
        "# Convert TF-IDF vectors to PyTorch tensors\n",
        "X = torch.tensor(sentence_vectors, dtype=torch.float32)\n",
        "y = torch.tensor([1 if i < 3 else 0 for i in range(len(sentences))], dtype=torch.float32)  # Assume first 3 sentences are important\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = lstm_model(X.unsqueeze(1))\n",
        "    loss = criterion(outputs.squeeze(), y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8b246ff3-eddd-4126-9dc4-265b91dfef1f",
      "metadata": {
        "id": "8b246ff3-eddd-4126-9dc4-265b91dfef1f",
        "outputId": "b870806e-53af-4678-f6c9-b8c8aba82e50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Extractive Summary (LSTM):\n",
            " Sure, too much tech can make life trickier, not easier. No, that's not going to stop us listing some of the coolest gadgets, gizmos and accessories that could just make you the happiest traveler this side of the Apple store. (CNN) -- Yes, it's great to travel light.\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    scores = lstm_model(X.unsqueeze(1)).squeeze()\n",
        "    top_indices = torch.argsort(scores, descending=True)[:3]\n",
        "\n",
        "extractive_summary_lstm = [sentences[i] for i in top_indices]\n",
        "print(\"\\n🔹 Extractive Summary (LSTM):\\n\", \" \".join(extractive_summary_lstm))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bec2406a-c2ab-4d52-b367-47eb19e88efe",
      "metadata": {
        "id": "bec2406a-c2ab-4d52-b367-47eb19e88efe"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a6f9e577-a4ac-4eb4-bb68-50a9503eb82d",
      "metadata": {
        "id": "a6f9e577-a4ac-4eb4-bb68-50a9503eb82d",
        "outputId": "e89208f6-4f1c-4b19-a929-d1b99e138d71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT Embeddings Shape: torch.Size([104, 768])\n"
          ]
        }
      ],
      "source": [
        "def get_bert_embeddings(sentences):\n",
        "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :]  # Extract CLS token embeddings\n",
        "\n",
        "sentence_embeddings = get_bert_embeddings(sentences)\n",
        "print(\"\\nBERT Embeddings Shape:\", sentence_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ea391e33-bfb9-40cf-bc20-34e82f8ba444",
      "metadata": {
        "id": "ea391e33-bfb9-40cf-bc20-34e82f8ba444",
        "outputId": "8936bcda-1fa5-458d-d22f-7c97e7a16938",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Extractive Summary (BERT):\n",
            " This towel was reportedly developed for military use, made from a woven polyurethane material that sand won't stick to. It's part of the company's Guaranteed on Board program, ensuring acceptance on Delta, Southwest and most other major airlines. This multipurpose suitcase is designed for businessmen making speeches on short notice.\n"
          ]
        }
      ],
      "source": [
        "sentence_scores = torch.mean(sentence_embeddings, dim=1)  # Compute sentence importance\n",
        "\n",
        "top_indices = torch.argsort(sentence_scores, descending=True)[:3]\n",
        "extractive_summary_bert = [sentences[i] for i in top_indices]\n",
        "\n",
        "print(\"\\n🔹 Extractive Summary (BERT):\\n\", \" \".join(extractive_summary_bert))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "92dd6483-c6ae-4f23-b748-adbc93965177",
      "metadata": {
        "id": "92dd6483-c6ae-4f23-b748-adbc93965177",
        "outputId": "57f596da-d8e2-4208-b65b-e7e4ca0905de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ROUGE Scores (LSTM): {'rouge1': Score(precision=0.12244897959183673, recall=0.09375, fmeasure=0.10619469026548671), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.08163265306122448, recall=0.0625, fmeasure=0.07079646017699115)}\n",
            "\n",
            " ROUGE Scores (BERT): {'rouge1': Score(precision=0.1320754716981132, recall=0.109375, fmeasure=0.11965811965811966), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.09433962264150944, recall=0.078125, fmeasure=0.08547008547008547)}\n"
          ]
        }
      ],
      "source": [
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "\n",
        "# Evaluate LSTM summary\n",
        "rouge_lstm = scorer.score(sample_summary, \" \".join(extractive_summary_lstm))\n",
        "print(\"\\n ROUGE Scores (LSTM):\", rouge_lstm)\n",
        "\n",
        "# Evaluate BERT summary\n",
        "rouge_bert = scorer.score(sample_summary, \" \".join(extractive_summary_bert))\n",
        "print(\"\\n ROUGE Scores (BERT):\", rouge_bert)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8f6916f8-6b3b-4693-9a4a-3fd69927f599",
      "metadata": {
        "id": "8f6916f8-6b3b-4693-9a4a-3fd69927f599",
        "outputId": "20ed3bf1-887c-497e-d958-7ce73e848dc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `2.0` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " **Generated Abstractive Summary (T5):**\n",
            "a five-megapixel camera clips onto your clothes and does the work for you. a washable mark-mat is a great time-waster for kids to draw on. a wireless version ($99.95) syncs with your smart phone or other Bluetooth-enabled device.\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "model_name = \"t5-small\"  # Change to \"t5-base\" or \"t5-large\" for better results\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def generate_abstractive_summary(text, max_input_length=512, max_output_length=150):\n",
        "    # Prepend \"summarize: \" to the input text\n",
        "    input_text = \"summarize: \" + text\n",
        "\n",
        "    # Tokenize and truncate input\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Generate summary\n",
        "    summary_ids = model.generate(inputs.input_ids, max_length=max_output_length, min_length=50, length_penalty=2.0)\n",
        "\n",
        "    # Decode summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Example usage on an article\n",
        "sample_article = undersampled_data[0]['article']  # Selecting one article from undersampled dataset\n",
        "generated_summary_t5 = generate_abstractive_summary(sample_article)\n",
        "\n",
        "print(\"\\n **Generated Abstractive Summary (T5):**\")\n",
        "print(generated_summary_t5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d8bfc536-f7e7-4bb6-941b-cceef5fc46f3",
      "metadata": {
        "id": "d8bfc536-f7e7-4bb6-941b-cceef5fc46f3",
        "outputId": "043e3644-eec7-4b4a-da1e-4642021936f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " **Generated Abstractive Summary (GPT-2):**\n",
            "travel light. It's great to travel light. Sure, too much tech can make life trickier, not easier. Yes, it's great to travel light. Yes, it's great to travel light. Sure, too much tech can make life trickier, not easier. no, that's not going to stop us listing some of the coolest gadgets, gizmos and accessories that could just make you the happiest traveler this side\n"
          ]
        }
      ],
      "source": [
        "def generate_gpt2_summary(text, max_input_length=300, max_new_tokens=100):\n",
        "    # Tokenize and truncate input\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Generate summary with corrected parameters\n",
        "    summary_ids = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_new_tokens=max_new_tokens,  # Controls only the newly generated text\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50\n",
        "    )\n",
        "\n",
        "    # Decode summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Example usage\n",
        "generated_summary_gpt = generate_gpt2_summary(sample_article)\n",
        "\n",
        "print(\"\\n **Generated Abstractive Summary (GPT-2):**\")\n",
        "print(generated_summary_gpt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a54da11e-e42d-4de3-8a82-e8018ddcd562",
      "metadata": {
        "id": "a54da11e-e42d-4de3-8a82-e8018ddcd562",
        "outputId": "622708aa-93f7-4784-b591-5b30b72e9bf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of LSTM Summary: <class 'list'>\n",
            "Type of BERT Summary: <class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(\"Type of LSTM Summary:\", type(extractive_summary_lstm))\n",
        "print(\"Type of BERT Summary:\", type(extractive_summary_bert))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6275eea5-5be9-4518-a59e-5fdec39f89cc",
      "metadata": {
        "id": "6275eea5-5be9-4518-a59e-5fdec39f89cc"
      },
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "def compute_rouge_scores(reference_summary, generated_summary):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    scores = scorer.score(reference_summary, generated_summary)\n",
        "\n",
        "    # Extract F1 scores\n",
        "    rouge1 = scores[\"rouge1\"].fmeasure\n",
        "    rouge2 = scores[\"rouge2\"].fmeasure\n",
        "    rougeL = scores[\"rougeL\"].fmeasure\n",
        "\n",
        "    return {\"ROUGE-1\": rouge1, \"ROUGE-2\": rouge2, \"ROUGE-L\": rougeL}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "61b29497-499f-4f81-9441-b34471b85020",
      "metadata": {
        "id": "61b29497-499f-4f81-9441-b34471b85020"
      },
      "outputs": [],
      "source": [
        "extractive_summary_lstm = str(extractive_summary_lstm).strip()\n",
        "extractive_summary_bert = str(extractive_summary_bert).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "cfb95038-9494-449c-8916-5d8a54844d41",
      "metadata": {
        "id": "cfb95038-9494-449c-8916-5d8a54844d41"
      },
      "outputs": [],
      "source": [
        "# Reference summary (Ground truth from dataset)\n",
        "reference_summary = sample_summary # The actual summary from CNN/DailyMail dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compute ROUGE scores for each model\n",
        "rouge_lstm = compute_rouge_scores(reference_summary, extractive_summary_lstm)\n",
        "rouge_bert = compute_rouge_scores(reference_summary, extractive_summary_bert)\n",
        "rouge_t5 = compute_rouge_scores(reference_summary, generated_summary_t5)\n",
        "rouge_gpt = compute_rouge_scores(reference_summary, generated_summary_gpt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "29ea67c7-6a07-49bc-89a1-a157e9e55936",
      "metadata": {
        "id": "29ea67c7-6a07-49bc-89a1-a157e9e55936",
        "outputId": "441bf2bc-a124-4e0a-8633-308f4ba0fd9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROUGE Scores Comparison:\n",
            " Extractive LSTM: {'ROUGE-1': 0.10619469026548671, 'ROUGE-2': 0.0, 'ROUGE-L': 0.07079646017699115}\n",
            " Extractive BERT: {'ROUGE-1': 0.11965811965811966, 'ROUGE-2': 0.0, 'ROUGE-L': 0.08547008547008547}\n",
            " Abstractive T5: {'ROUGE-1': 0.22429906542056074, 'ROUGE-2': 0.03809523809523809, 'ROUGE-L': 0.16822429906542055}\n",
            " Abstractive GPT: {'ROUGE-1': 0.10526315789473685, 'ROUGE-2': 0.0, 'ROUGE-L': 0.09022556390977443}\n"
          ]
        }
      ],
      "source": [
        "# Print ROUGE scores for each model\n",
        "print(\"\\nROUGE Scores Comparison:\")\n",
        "print(f\" Extractive LSTM: {rouge_lstm}\")\n",
        "print(f\" Extractive BERT: {rouge_bert}\")\n",
        "print(f\" Abstractive T5: {rouge_t5}\")\n",
        "print(f\" Abstractive GPT: {rouge_gpt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "55bc4ea3-8607-4c7d-8fbe-a1d130b30a3a",
      "metadata": {
        "id": "55bc4ea3-8607-4c7d-8fbe-a1d130b30a3a",
        "outputId": "e4a7c59f-cc9c-4ae9-ef59-fa2649de0179",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROUGE Scores Comparison:\n",
            "Extractive LSTM: {'ROUGE-1': 0.10619469026548671, 'ROUGE-2': 0.0, 'ROUGE-L': 0.07079646017699115}\n",
            "Extractive BERT: {'ROUGE-1': 0.11965811965811966, 'ROUGE-2': 0.0, 'ROUGE-L': 0.08547008547008547}\n",
            "Abstractive T5: {'ROUGE-1': 0.22429906542056074, 'ROUGE-2': 0.03809523809523809, 'ROUGE-L': 0.16822429906542055}\n",
            "Abstractive GPT: {'ROUGE-1': 0.10526315789473685, 'ROUGE-2': 0.0, 'ROUGE-L': 0.09022556390977443}\n",
            "\n",
            "Best Model per ROUGE Score:\n",
            "ROUGE-1: Abstractive T5 with score 0.2243\n",
            "ROUGE-2: Abstractive T5 with score 0.0381\n",
            "ROUGE-L: Abstractive T5 with score 0.1682\n",
            "\n",
            "Overall Best Model: Abstractive T5 with an average ROUGE score of 0.1435\n",
            "\n",
            "Extractive vs. Abstractive Comparison:\n",
            "Average ROUGE Score (Extractive Models): 0.0637\n",
            "Average ROUGE Score (Abstractive Models): 0.1044\n",
            "Abstractive models perform better based on ROUGE scores.\n"
          ]
        }
      ],
      "source": [
        "# Print results\n",
        "print(\"\\nROUGE Scores Comparison:\")\n",
        "print(f\"Extractive LSTM: {rouge_lstm}\")\n",
        "print(f\"Extractive BERT: {rouge_bert}\")\n",
        "print(f\"Abstractive T5: {rouge_t5}\")\n",
        "print(f\"Abstractive GPT: {rouge_gpt}\")\n",
        "\n",
        "# Compare models based on ROUGE scores\n",
        "models_rouge = {\n",
        "    \"Extractive LSTM\": rouge_lstm,\n",
        "    \"Extractive BERT\": rouge_bert,\n",
        "    \"Abstractive T5\": rouge_t5,\n",
        "    \"Abstractive GPT\": rouge_gpt\n",
        "}\n",
        "\n",
        "# Find the best model for each ROUGE metric\n",
        "best_rouge1 = max(models_rouge, key=lambda x: models_rouge[x]['ROUGE-1'])\n",
        "best_rouge2 = max(models_rouge, key=lambda x: models_rouge[x]['ROUGE-2'])\n",
        "best_rougeL = max(models_rouge, key=lambda x: models_rouge[x]['ROUGE-L'])\n",
        "\n",
        "print(\"\\nBest Model per ROUGE Score:\")\n",
        "print(f\"ROUGE-1: {best_rouge1} with score {models_rouge[best_rouge1]['ROUGE-1']:.4f}\")\n",
        "print(f\"ROUGE-2: {best_rouge2} with score {models_rouge[best_rouge2]['ROUGE-2']:.4f}\")\n",
        "print(f\"ROUGE-L: {best_rougeL} with score {models_rouge[best_rougeL]['ROUGE-L']:.4f}\")\n",
        "\n",
        "# Determine overall best model (average ROUGE scores)\n",
        "average_rouge = {model: sum(scores.values()) / len(scores) for model, scores in models_rouge.items()}\n",
        "best_model_overall = max(average_rouge, key=average_rouge.get)\n",
        "\n",
        "print(f\"\\nOverall Best Model: {best_model_overall} with an average ROUGE score of {average_rouge[best_model_overall]:.4f}\")\n",
        "\n",
        "# Compare Extractive vs Abstractive models\n",
        "avg_extractive = (average_rouge[\"Extractive LSTM\"] + average_rouge[\"Extractive BERT\"]) / 2\n",
        "avg_abstractive = (average_rouge[\"Abstractive T5\"] + average_rouge[\"Abstractive GPT\"]) / 2\n",
        "\n",
        "print(\"\\nExtractive vs. Abstractive Comparison:\")\n",
        "print(f\"Average ROUGE Score (Extractive Models): {avg_extractive:.4f}\")\n",
        "print(f\"Average ROUGE Score (Abstractive Models): {avg_abstractive:.4f}\")\n",
        "\n",
        "if avg_extractive > avg_abstractive:\n",
        "    print(\"Extractive models perform better based on ROUGE scores.\")\n",
        "else:\n",
        "    print(\"Abstractive models perform better based on ROUGE scores.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def compute_rouge_scores(reference, generated):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    return scorer.score(reference, generated)\n",
        "\n",
        "# Load Summarization Pipelines\n",
        "bert_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")  # BART (BERT-based)\n",
        "t5_summarizer = pipeline(\"summarization\", model=\"t5-small\")  # T5 model\n",
        "\n",
        "# Define batch sizes\n",
        "batch_sizes = [8, 16, 32]\n",
        "\n",
        "def evaluate_batch_sizes(summarizer, reference_summary, batch_sizes):\n",
        "    results = {}\n",
        "    for batch_size in batch_sizes:\n",
        "        try:\n",
        "            generated_summary_list = summarizer(reference_summary, batch_size=batch_size, truncation=True)\n",
        "            if not generated_summary_list:  # Check if list is empty\n",
        "                print(f\"Warning: No summary generated for batch size {batch_size}\")\n",
        "                continue\n",
        "\n",
        "            generated_summary = generated_summary_list[0].get('summary_text', '')  # Safe dictionary access\n",
        "            if not generated_summary:  # Check for empty summary\n",
        "                print(f\"Warning: Empty summary for batch size {batch_size}\")\n",
        "                continue\n",
        "\n",
        "            scores = compute_rouge_scores(reference_summary, generated_summary)\n",
        "            results[batch_size] = scores\n",
        "        except Exception as e:\n",
        "            print(f\"Error with batch size {batch_size}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Select a sample article and its summary\n",
        "sample_article = undersampled_data[0][\"article\"]\n",
        "reference_summary = undersampled_data[0][\"highlights\"]\n",
        "\n",
        "# Run evaluation\n",
        "rouge_bert_batches = evaluate_batch_sizes(bert_summarizer, reference_summary, batch_sizes)\n",
        "rouge_t5_batches = evaluate_batch_sizes(t5_summarizer, reference_summary, batch_sizes)\n",
        "\n",
        "# Print Results\n",
        "print(\"\\n🔹 BERT-Based (BART) ROUGE Scores by Batch Size:\")\n",
        "for batch, scores in rouge_bert_batches.items():\n",
        "    print(f\"Batch {batch}: ROUGE-1: {scores['rouge1'].fmeasure:.4f}, ROUGE-2: {scores['rouge2'].fmeasure:.4f}, ROUGE-L: {scores['rougeL'].fmeasure:.4f}\")\n",
        "\n",
        "print(\"\\n🔹 T5 ROUGE Scores by Batch Size:\")\n",
        "for batch, scores in rouge_t5_batches.items():\n",
        "    print(f\"Batch {batch}: ROUGE-1: {scores['rouge1'].fmeasure:.4f}, ROUGE-2: {scores['rouge2'].fmeasure:.4f}, ROUGE-L: {scores['rougeL'].fmeasure:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oynWq4wQjQnW",
        "outputId": "4ce65219-f276-42b6-fbd7-512c3c68fc27"
      },
      "id": "oynWq4wQjQnW",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Your max_length is set to 142, but your input_length is only 88. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
            "Your max_length is set to 142, but your input_length is only 88. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
            "Your max_length is set to 142, but your input_length is only 88. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
            "Your max_length is set to 200, but your input_length is only 105. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n",
            "Your max_length is set to 200, but your input_length is only 105. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n",
            "Your max_length is set to 200, but your input_length is only 105. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 BERT-Based (BART) ROUGE Scores by Batch Size:\n",
            "Batch 8: ROUGE-1: 0.8364, ROUGE-2: 0.8333, ROUGE-L: 0.8364\n",
            "Batch 16: ROUGE-1: 0.8364, ROUGE-2: 0.8333, ROUGE-L: 0.8364\n",
            "Batch 32: ROUGE-1: 0.8364, ROUGE-2: 0.8333, ROUGE-L: 0.8364\n",
            "\n",
            "🔹 T5 ROUGE Scores by Batch Size:\n",
            "Batch 8: ROUGE-1: 0.6087, ROUGE-2: 0.6000, ROUGE-L: 0.6087\n",
            "Batch 16: ROUGE-1: 0.6087, ROUGE-2: 0.6000, ROUGE-L: 0.6087\n",
            "Batch 32: ROUGE-1: 0.6087, ROUGE-2: 0.6000, ROUGE-L: 0.6087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "274c4ebf-4fb3-4194-9e7a-1992c8fb4a75",
      "metadata": {
        "id": "274c4ebf-4fb3-4194-9e7a-1992c8fb4a75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d3a49df-eee6-4625-ae83-19399c221e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Fine-tuning BART with SGD\n",
            "Epoch 1, Loss: 9.1117\n",
            "\n",
            "🔹 Fine-tuning T5 with SGD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 16.7363\n",
            "\n",
            "🔹 Fine-tuning BART with Adam\n",
            "Epoch 1, Loss: 9.0800\n",
            "\n",
            "🔹 Fine-tuning T5 with Adam\n",
            "Epoch 1, Loss: 15.9017\n",
            "\n",
            "🔹 ROUGE Scores for Different Optimizers:\n",
            "\n",
            "BART_SGD:\n",
            "  ROUGE-1: 0.3441\n",
            "  ROUGE-2: 0.1758\n",
            "  ROUGE-L: 0.2581\n",
            "\n",
            "T5_SGD:\n",
            "  ROUGE-1: 0.3611\n",
            "  ROUGE-2: 0.2286\n",
            "  ROUGE-L: 0.3333\n",
            "\n",
            "BART_Adam:\n",
            "  ROUGE-1: 0.4706\n",
            "  ROUGE-2: 0.2410\n",
            "  ROUGE-L: 0.3529\n",
            "\n",
            "T5_Adam:\n",
            "  ROUGE-1: 0.3611\n",
            "  ROUGE-2: 0.2286\n",
            "  ROUGE-L: 0.3333\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.optim import SGD, Adam\n",
        "from transformers import BartForConditionalGeneration, T5ForConditionalGeneration, BartTokenizer, T5Tokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")[\"train\"]\n",
        "sample_article = dataset[0][\"article\"][:256]  # Reduce max tokens\n",
        "reference_summary = dataset[0][\"highlights\"][:256]  # Reduce max tokens\n",
        "\n",
        "# Load Tokenizers\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Load Models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
        "\n",
        "# ROUGE Score Function\n",
        "def compute_rouge_scores(reference, generated):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    return scorer.score(reference, generated)\n",
        "\n",
        "# Training Function\n",
        "def fine_tune_and_evaluate(model, tokenizer, optimizer_type, num_epochs=1):  # Reduce epochs\n",
        "    optimizer = optimizer_type(model.parameters(), lr=5e-5)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        torch.cuda.empty_cache()  # Prevent memory overflow\n",
        "\n",
        "        inputs = tokenizer(sample_article, return_tensors=\"pt\", max_length=256, truncation=True, padding=\"max_length\")\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "        labels = tokenizer(reference_summary, return_tensors=\"pt\", max_length=150, truncation=True, padding=\"max_length\").input_ids.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Evaluate model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(sample_article, return_tensors=\"pt\", max_length=256, truncation=True).to(device)\n",
        "        summary_ids = model.generate(inputs.input_ids, max_length=150, num_beams=5, early_stopping=True)\n",
        "        generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return compute_rouge_scores(reference_summary, generated_summary)\n",
        "\n",
        "# Fine-tune and evaluate with SGD & Adam\n",
        "optimizers = {\"SGD\": SGD, \"Adam\": Adam}\n",
        "results = {}\n",
        "\n",
        "for optimizer_name, optimizer in optimizers.items():\n",
        "    print(f\"\\n🔹 Fine-tuning BART with {optimizer_name}\")\n",
        "    results[f\"BART_{optimizer_name}\"] = fine_tune_and_evaluate(bart_model, bart_tokenizer, optimizer)\n",
        "\n",
        "    print(f\"\\n🔹 Fine-tuning T5 with {optimizer_name}\")\n",
        "    results[f\"T5_{optimizer_name}\"] = fine_tune_and_evaluate(t5_model, t5_tokenizer, optimizer)\n",
        "\n",
        "# Print Results\n",
        "print(\"\\n🔹 ROUGE Scores for Different Optimizers:\")\n",
        "for model_optimizer, scores in results.items():\n",
        "    print(f\"\\n{model_optimizer}:\")\n",
        "    print(f\"  ROUGE-1: {scores['rouge1'].fmeasure:.4f}\")\n",
        "    print(f\"  ROUGE-2: {scores['rouge2'].fmeasure:.4f}\")\n",
        "    print(f\"  ROUGE-L: {scores['rougeL'].fmeasure:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LtdT7iFMeksf"
      },
      "id": "LtdT7iFMeksf",
      "execution_count": 24,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}